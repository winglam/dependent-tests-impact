\section{Evaluation}
\label{sec:evaluation}

To reiterate the goal of the paper is to determine whether test dependence can affect test prioritization, to study the extent of the impact and how we should augment these existing techniques to deal with dependent tests.

To achieve our goal we studied five real-world programs. The programs we analyzed were Jodatime, XML Security, Crystal, Synoptic and JFreeChart 

\textbf{miss a table here.}

The reason why we chose to apply test prioritization to these programs is because they are known to contain dependent tests [1]. Therefore if these techniques generates an order where its dependent tests will be revealed when the test suite is ran, then the importance of dependent tests in regression testing techniques will be solidified.

\subsection{Experimental Procedure}

We conducted the following experimental procedure on each of the five real-world programs we studied:

\begin{enumerate}
\item Establish an original intended order. We establish one by running the tests with no specified order. If the tests all pass then we treat the order that was just used as our original intended order. Otherwise we use our tools to generate an arbitrary order for all the tests. 
\item Instrument all classes that are not tests so that each statement and function that gets executed is recorded.
\item Execute the test cases that we would want to generate a test prioritization order for to obtain the number of statements and functions executed by each of those test cases. The test execution results from this step will serve as the data we collect for No Prioritization. 
\item Generate a test prioritization order for the test cases. 
\item Execute the tests cases again but in the order generated from the previous step.
\item Observe the results and record which tests passed, failed or resulted in an error.
\item With the exception of the No Prioritization technique, repeat steps four to six with each of the test prioritization techniques listed in Table 1.
\end{enumerate}


\textbf{MIss a table here}

No prioritization is simply the application of no technique. The order generated by this is the order intended by the developers of the test suites and this will serve as a control for whether a test is expected to pass, fail or result in an error.

\subsection{Findings}

Similar to JUnit [10], the distinction between tests that fail and tests that results in errors is that failures are when our test cases fail, in other words the tests' assertions were incorrect. On the other hand, errors are unexpected interruptions that occurred during the tests' execution from anomalies such as exceptions.

\textbf{miss a table here}

\textbf{Whether test dependence can affect test prioritization.} By observing the results of a test suite executed in test prioritization order and its original intended order, we can determine whether test dependence can affect test prioritization. 

When test prioritization absolute statement coverage was applied to Synoptic's test suite, one of Synoptic's dependent tests was revealed. The absolute statement coverage technique on Synoptic resulted in one additional test failing compared to the other test execution results of Synoptic. Furthermore, every variation of test execution order we generated for Crystal and JFreeChart resulted in a different number of tests that pass, fail, and received errors compared to its respective no prioritization execution results. Therefore based on the inconsistent test execution results from Crystal, JFreeChart and Synoptic, we have shown concrete evidence that test dependence can affect the results of test executions after test prioritization techniques have been applied.

\textbf{Extent of the impact of dependent tests.} In order to study the extent of the impact, we can measure how often dependent tests can affect a testing technique. For example, if ten percent of a test suite are dependent tests, after applying five test prioritization techniques to that test suite, if none of the technique's output is affected we may say that the impact is minimal.  On the other hand, if all five outputs are affected, we may conclude that the impact of dependent tests are non-trivial for test prioritization techniques. 
Despite the different test execution orders used for each technique, Jodatime's and XML-Security's test executions all resulted in the same tests passing, failing, and receiving errors. Therefore based on the consistent test execution results from Jodatime, and XML-security, we conclude that the extent of the impact of test dependence on test prioritization is minimal.

\textbf{How we should augment existing techniques to deal with dependent tests.} We plan to augment test prioritization techniques to deal with dependent tests by "merging" two dependent tests as a single test, and measure their coverage together. In other words, we will always execute dependent tests together with their original intended order. 
In order for this to be possible we intend to improve our tools so it can identify which tests are dependent on one another. With the list of which tests are dependent on one another, we can augment the TestListGenerator tool to prioritize the dependent tests together and ensure that they are executed in their original intended order. 


\subsection{Threats to Validity}
Our findings are based on five programs that are known to contain dependent tests and may not apply to arbitrary programs. The applications we studied are all written in Java and have JUnit test suites. The original intended order of a test suite is defined to be any order in which all tests of the test suite pass. If no such order exists an arbitrary order is generated. However neither of these orders may be the order developers actually intend for the tests to be executed in.  
 

Another threat to validity is due to how JUnit uses reflection to select the order in which tests of a test suite are executed, tests that are executed first may contain extra coverage elements. This is because the constructors of the variables those tests utilize may be counted as part of the tests. 

\textbf{miss code example here explanation here}
